{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7674705,"sourceType":"datasetVersion","datasetId":4476797}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\nfrom kagglehub import KaggleDatasetAdapter\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load dataset from Kaggle\nfile_path = \"dataset_invade.csv\"\ndf = kagglehub.load_dataset(\n    KaggleDatasetAdapter.PANDAS,\n    \"bobaaayoung/dataset-invade\",\n    file_path,\n)\n\n# Display first few rows to verify loading\nprint(\"First 5 records:\")\nprint(df.head())\n\n# Preprocessing Steps\n# 1. Convert categorical features into numerical using one-hot encoding\ndf_cleaned = pd.get_dummies(df, columns=['protocol_type', 'service', 'flag'], drop_first=True)\n\n# 2. Encode target variable: 'No' -> 0, 'Yes' -> 1\ndf_cleaned['attack'] = df_cleaned['attack'].map({'No': 0, 'Yes': 1})\n\n# 3. Separate features (X) and target variable (y)\nX = df_cleaned.drop(columns=['attack'])\ny = df_cleaned['attack']\n\n# 4. Normalize numerical features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 5. Split dataset into training (80%) and testing (20%) sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n\n# Display dataset shapes after preprocessing\nprint(f\"Training set size: {X_train.shape}\")\nprint(f\"Testing set size: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T10:38:14.934409Z","iopub.execute_input":"2025-03-23T10:38:14.934766Z","iopub.status.idle":"2025-03-23T10:38:16.807776Z","shell.execute_reply.started":"2025-03-23T10:38:14.934707Z","shell.execute_reply":"2025-03-23T10:38:16.806612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install dependencies as needed:\n# pip install kagglehub[pandas-datasets] scikit-learn\n\nimport kagglehub\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n\n# Load dataset\nfile_path = \"dataset_invade.csv\"\ndf = kagglehub.load_dataset(\n    KaggleDatasetAdapter.PANDAS,\n    \"bobaaayoung/dataset-invade\",\n    file_path,\n)\n\n# Check for missing values\nprint(\"Missing values per column:\")\nprint(df.isnull().sum())\n\n# Encode target variable\ndf['attack'] = df['attack'].map({'Yes': 1, 'No': 0})\n\n# Split features and target\nX = df.drop('attack', axis=1)\ny = df['attack']\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Define preprocessing steps\ncategorical_cols = ['protocol_type', 'service', 'flag']\nnumerical_cols = [col for col in X.columns if col not in categorical_cols]\n\n# Preprocessor for Logistic Regression (with scaling)\npreprocessor_lr = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n        ('num', StandardScaler(), numerical_cols)\n    ])\n\n# Preprocessor for Random Forest (without scaling)\npreprocessor_rf = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n    ],\n    remainder='passthrough'\n)\n\n# Create pipelines\npipeline_lr = Pipeline([\n    ('preprocessor', preprocessor_lr),\n    ('classifier', LogisticRegression(\n        class_weight='balanced',\n        max_iter=1000,\n        random_state=42\n    ))\n])\n\npipeline_rf = Pipeline([\n    ('preprocessor', preprocessor_rf),\n    ('classifier', RandomForestClassifier(\n        n_estimators=100,\n        class_weight='balanced',\n        random_state=42\n    ))\n])\n\n# Train models\nprint(\"\\nTraining Logistic Regression...\")\npipeline_lr.fit(X_train, y_train)\n\nprint(\"\\nTraining Random Forest...\")\npipeline_rf.fit(X_train, y_train)\n\n# Generate predictions\ny_pred_lr = pipeline_lr.predict(X_test)\ny_pred_proba_lr = pipeline_lr.predict_proba(X_test)[:, 1]\n\ny_pred_rf = pipeline_rf.predict(X_test)\ny_pred_proba_rf = pipeline_rf.predict_proba(X_test)[:, 1]\n\n# Evaluate models\ndef evaluate_model(name, y_true, y_pred, y_proba):\n    print(f\"\\n{name} Evaluation:\")\n    print(\"Classification Report:\")\n    print(classification_report(y_true, y_pred))\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_true, y_pred))\n    print(f\"ROC AUC Score: {roc_auc_score(y_true, y_proba):.4f}\")\n\nevaluate_model(\"Logistic Regression\", y_test, y_pred_lr, y_pred_proba_lr)\nevaluate_model(\"Random Forest\", y_test, y_pred_rf, y_pred_proba_rf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T10:38:20.927569Z","iopub.execute_input":"2025-03-23T10:38:20.927970Z","iopub.status.idle":"2025-03-23T10:39:00.856453Z","shell.execute_reply.started":"2025-03-23T10:38:20.927927Z","shell.execute_reply":"2025-03-23T10:39:00.855346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install dependencies as needed:\n# pip install kagglehub[pandas-datasets] scikit-learn pandas numpy\n\nimport kagglehub\nimport pandas as pd\nfrom kagglehub import KaggleDatasetAdapter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n\n# Load dataset\nfile_path = \"dataset_invade.csv\"\ndf = kagglehub.load_dataset(\n    KaggleDatasetAdapter.PANDAS,\n    \"bobaaayoung/dataset-invade\",\n    file_path,\n)\n\n# Check for missing values\nprint(\"Missing values per column:\")\nprint(df.isnull().sum())\n\n# ----------------------------\n# Preprocessing Steps\n# ----------------------------\n\n# 1. Convert categorical features into numerical using one-hot encoding\ndf_cleaned = pd.get_dummies(df, columns=['protocol_type', 'service', 'flag'], drop_first=True)\n\n# 2. Encode target variable: 'No' -> 0, 'Yes' -> 1\ndf_cleaned['attack'] = df_cleaned['attack'].map({'No': 0, 'Yes': 1})\n\n# 3. Separate features (X) and target variable (y)\nX = df_cleaned.drop(columns=['attack'])\ny = df_cleaned['attack']\n\n# 4. Normalize numerical features using StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 5. Split dataset into training (80%) and testing (20%) sets (stratified on y)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Display dataset shapes after preprocessing\nprint(f\"Training set size: {X_train.shape}\")\nprint(f\"Testing set size: {X_test.shape}\")\n\n# ----------------------------\n# Model Training and Evaluation\n# ----------------------------\n\n# Initialize models\nlog_reg = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\nrf_clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n\n# Train Logistic Regression\nprint(\"\\nTraining Logistic Regression...\")\nlog_reg.fit(X_train, y_train)\ny_pred_lr = log_reg.predict(X_test)\ny_pred_proba_lr = log_reg.predict_proba(X_test)[:, 1]\n\n# Train Random Forest\nprint(\"\\nTraining Random Forest...\")\nrf_clf.fit(X_train, y_train)\ny_pred_rf = rf_clf.predict(X_test)\ny_pred_proba_rf = rf_clf.predict_proba(X_test)[:, 1]\n\n# Evaluation function\ndef evaluate_model(name, y_true, y_pred, y_proba):\n    print(f\"\\n{name} Evaluation:\")\n    print(\"Classification Report:\")\n    print(classification_report(y_true, y_pred))\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_true, y_pred))\n    print(f\"ROC AUC Score: {roc_auc_score(y_true, y_proba):.4f}\")\n\n# Evaluate models\nevaluate_model(\"Logistic Regression\", y_test, y_pred_lr, y_pred_proba_lr)\nevaluate_model(\"Random Forest\", y_test, y_pred_rf, y_pred_proba_rf)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:22:38.615356Z","iopub.execute_input":"2025-03-23T11:22:38.615983Z","iopub.status.idle":"2025-03-23T11:23:00.360088Z","shell.execute_reply.started":"2025-03-23T11:22:38.615915Z","shell.execute_reply":"2025-03-23T11:23:00.358884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install dependencies as needed:\n# pip install kagglehub[pandas-datasets] scikit-learn pandas numpy\n\nimport kagglehub\nimport pandas as pd\nfrom kagglehub import KaggleDatasetAdapter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier  # New import for KNN\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n\n# Load dataset\nfile_path = \"dataset_invade.csv\"\ndf = kagglehub.load_dataset(\n    KaggleDatasetAdapter.PANDAS,\n    \"bobaaayoung/dataset-invade\",\n    file_path,\n)\n\n# Check for missing values\nprint(\"Missing values per column:\")\nprint(df.isnull().sum())\n\n# ----------------------------\n# Preprocessing Steps\n# ----------------------------\n\n# 1. Convert categorical features into numerical using one-hot encoding\ndf_cleaned = pd.get_dummies(df, columns=['protocol_type', 'service', 'flag'], drop_first=True)\n\n# 2. Encode target variable: 'No' -> 0, 'Yes' -> 1\ndf_cleaned['attack'] = df_cleaned['attack'].map({'No': 0, 'Yes': 1})\n\n# 3. Separate features (X) and target variable (y)\nX = df_cleaned.drop(columns=['attack'])\ny = df_cleaned['attack']\n\n# 4. Normalize numerical features using StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 5. Split dataset into training (80%) and testing (20%) sets (stratified on y)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Display dataset shapes after preprocessing\nprint(f\"Training set size: {X_train.shape}\")\nprint(f\"Testing set size: {X_test.shape}\")\n\n# ----------------------------\n# Model Training and Evaluation\n# ----------------------------\n\n# Initialize models\nlog_reg = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\nrf_clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\nknn_clf = KNeighborsClassifier(n_neighbors=5)  # New KNN classifier with default parameters\n\n# Train Logistic Regression\nprint(\"\\nTraining Logistic Regression...\")\nlog_reg.fit(X_train, y_train)\ny_pred_lr = log_reg.predict(X_test)\ny_pred_proba_lr = log_reg.predict_proba(X_test)[:, 1]\n\n# Train Random Forest\nprint(\"\\nTraining Random Forest...\")\nrf_clf.fit(X_train, y_train)\ny_pred_rf = rf_clf.predict(X_test)\ny_pred_proba_rf = rf_clf.predict_proba(X_test)[:, 1]\n\n# Train K-Nearest Neighbors\nprint(\"\\nTraining K-Nearest Neighbors...\")\nknn_clf.fit(X_train, y_train)\ny_pred_knn = knn_clf.predict(X_test)\n# For KNN, use predict_proba for ROC AUC if available\ny_pred_proba_knn = knn_clf.predict_proba(X_test)[:, 1]\n\n# Evaluation function\ndef evaluate_model(name, y_true, y_pred, y_proba):\n    print(f\"\\n{name} Evaluation:\")\n    print(\"Classification Report:\")\n    print(classification_report(y_true, y_pred))\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_true, y_pred))\n    print(f\"ROC AUC Score: {roc_auc_score(y_true, y_proba):.4f}\")\n\n# Evaluate models\nevaluate_model(\"Logistic Regression\", y_test, y_pred_lr, y_pred_proba_lr)\nevaluate_model(\"Random Forest\", y_test, y_pred_rf, y_pred_proba_rf)\nevaluate_model(\"K-Nearest Neighbors\", y_test, y_pred_knn, y_pred_proba_knn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:24:26.762234Z","iopub.execute_input":"2025-03-23T11:24:26.762580Z","iopub.status.idle":"2025-03-23T11:25:17.891305Z","shell.execute_reply.started":"2025-03-23T11:24:26.762555Z","shell.execute_reply":"2025-03-23T11:25:17.890185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize models\nlog_reg = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\nrf_clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\nknn_clf = KNeighborsClassifier(n_neighbors=5) \n#first run\n# Training Logistic Regression\nprint(\"\\nTraining Logistic Regression...\")\nlog_reg.fit(X_train, y_train)\ny_pred_lr = log_reg.predict(X_test)\ny_pred_proba_lr = log_reg.predict_proba(X_test)[:, 1]\n\n# Training Random Forest\nprint(\"\\nTraining Random Forest...\")\nrf_clf.fit(X_train, y_train)\ny_pred_rf = rf_clf.predict(X_test)\ny_pred_proba_rf = rf_clf.predict_proba(X_test)[:, 1]\n\n# Training K-Nearest Neighbors\nprint(\"\\nTraining K-Nearest Neighbors...\")\nknn_clf.fit(X_train, y_train)\ny_pred_knn = knn_clf.predict(X_test)\ny_pred_proba_knn = knn_clf.predict_proba(X_test)[:, 1]\n\n# Evaluation function\ndef evaluate_model(name, y_true, y_pred, y_proba):\n    print(f\"\\n{name} Evaluation:\")\n    print(\"Classification Report:\")\n    print(classification_report(y_true, y_pred))\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_true, y_pred))\n    print(f\"ROC AUC Score: {roc_auc_score(y_true, y_proba):.4f}\")\n    \n# Evaluate models \nevaluate_model(\"Logistic Regression\", y_test, y_pred_lr, y_pred_proba_lr)\nevaluate_model(\"Random Forest\", y_test, y_pred_rf, y_pred_proba_rf)\nevaluate_model(\"K-Nearest Neighbors\", y_test, y_pred_knn, y_pred_proba_knn)\n\nimport kagglehub\nimport pandas as pd\nfrom kagglehub import KaggleDatasetAdapter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n#NOW WE ADD OUR DATASETS\n# firstly lets Load dataset \nfile_path = \"dataset_invade.csv\"\ndf = kagglehub.load_dataset(\n    KaggleDatasetAdapter.PANDAS,\n    \"bobaaayoung/dataset-invade\",\n    file_path,\n)\n\nprint(\"Missing values per column:\")\nprint(df.isnull().sum())\n#lets process first\n# 1. Convert categorical features into numerical using one-hot encoding\ndf_cleaned = pd.get_dummies(df, columns=['protocol_type', 'service', 'flag'], drop_first=True)\n\n# 2. Encode target variable: 'No' -> 0, 'Yes' -> 1\ndf_cleaned['attack'] = df_cleaned['attack'].map({'No': 0, 'Yes': 1})\n\n# 3. Separate features (X) and target variable (y)\nX = df_cleaned.drop(columns=['attack'])\ny = df_cleaned['attack']\n\n# 4. Normalize numerical features using StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 5. Split dataset into training (80%) and testing (20%) sets (stratified on y)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Display dataset shapes after preprocessing\nprint(f\"Training set size: {X_train.shape}\")\nprint(f\"Testing set size: {X_test.shape}\")\n\n#again initialise models\n# Initialize models\nlog_reg = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\nrf_clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\nknn_clf = KNeighborsClassifier(n_neighbors=5)  # New KNN classifier with default parameters\n# Initialize XGBoost Classifier\nxgb_clf = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n\n\n# Train Logistic Regression\nprint(\"\\nTraining Logistic Regression...\")\nlog_reg.fit(X_train, y_train)\ny_pred_lr = log_reg.predict(X_test)\ny_pred_proba_lr = log_reg.predict_proba(X_test)[:, 1]\n\n# Train Random Forest\nprint(\"\\nTraining Random Forest...\")\nrf_clf.fit(X_train, y_train)\ny_pred_rf = rf_clf.predict(X_test)\ny_pred_proba_rf = rf_clf.predict_proba(X_test)[:, 1]\n\n# Train K-Nearest Neighbors\nprint(\"\\nTraining K-Nearest Neighbors...\")\nknn_clf.fit(X_train, y_train)\ny_pred_knn = knn_clf.predict(X_test)\n# For KNN, use predict_proba for ROC AUC if available\ny_pred_proba_knn = knn_clf.predict_proba(X_test)[:, 1]\n\n# Train XGBoost\nprint(\"\\nTraining XGBoost...\")\nxgb_clf.fit(X_train, y_train)\ny_pred_xgb = xgb_clf.predict(X_test)\ny_pred_proba_xgb = xgb_clf.predict_proba(X_test)[:, 1]\n\n# Evaluation function\ndef evaluate_model(name, y_true, y_pred, y_proba):\n    print(f\"\\n{name} Evaluation:\")\n    print(\"Classification Report:\")\n    print(classification_report(y_true, y_pred))\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_true, y_pred))\n    print(f\"ROC AUC Score: {roc_auc_score(y_true, y_proba):.4f}\")\n#finally evaluate ALL models\n# Evaluate models\nevaluate_model(\"Logistic Regression\", y_test, y_pred_lr, y_pred_proba_lr)\nevaluate_model(\"Random Forest\", y_test, y_pred_rf, y_pred_proba_rf)\nevaluate_model(\"K-Nearest Neighbors\", y_test, y_pred_knn, y_pred_proba_knn)\nevaluate_model(\"XGBoost\", y_test, y_pred_xgb, y_pred_proba_xgb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T15:35:27.488535Z","iopub.execute_input":"2025-03-23T15:35:27.488911Z","iopub.status.idle":"2025-03-23T15:37:11.174458Z","shell.execute_reply.started":"2025-03-23T15:35:27.488876Z","shell.execute_reply":"2025-03-23T15:37:11.173362Z"}},"outputs":[{"name":"stdout","text":"\nTraining Logistic Regression...\n\nTraining Random Forest...\n\nTraining K-Nearest Neighbors...\n\nLogistic Regression Evaluation:\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.96      0.96     15411\n           1       0.96      0.94      0.95     14293\n\n    accuracy                           0.95     29704\n   macro avg       0.95      0.95      0.95     29704\nweighted avg       0.95      0.95      0.95     29704\n\nConfusion Matrix:\n[[14859   552]\n [  847 13446]]\nROC AUC Score: 0.9897\n\nRandom Forest Evaluation:\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      1.00     15411\n           1       1.00      0.99      1.00     14293\n\n    accuracy                           1.00     29704\n   macro avg       1.00      1.00      1.00     29704\nweighted avg       1.00      1.00      1.00     29704\n\nConfusion Matrix:\n[[15363    48]\n [   87 14206]]\nROC AUC Score: 0.9998\n\nK-Nearest Neighbors Evaluation:\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99     15411\n           1       0.99      0.99      0.99     14293\n\n    accuracy                           0.99     29704\n   macro avg       0.99      0.99      0.99     29704\nweighted avg       0.99      0.99      0.99     29704\n\nConfusion Matrix:\n[[15273   138]\n [  152 14141]]\nROC AUC Score: 0.9974\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-6-9b5c1e7a7800>:50: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n  df = kagglehub.load_dataset(\n","output_type":"stream"},{"name":"stdout","text":"Missing values per column:\nduration                  0\nprotocol_type             0\nservice                   0\nflag                      0\nsrc_bytes                 0\ndst_bytes                 0\nland                      0\nwrong_fragment            0\nurgent                    0\nhot                       0\nlogged_in                 0\nnum_compromised           0\ncount                     0\nsrv_count                 0\nserror_rate               0\nrerror_rate               0\nsame_srv_rate             0\ndiff_srv_rate             0\nsrv_diff_host_rate        0\ndst_host_count            0\ndst_host_srv_count        0\ndst_host_same_srv_rate    0\ndst_host_diff_srv_rate    0\nattack                    0\ndtype: int64\nTraining set size: (118813, 101)\nTesting set size: (29704, 101)\n\nTraining Logistic Regression...\n\nTraining Random Forest...\n\nTraining K-Nearest Neighbors...\n\nTraining XGBoost...\n\nLogistic Regression Evaluation:\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.96      0.96     15411\n           1       0.96      0.94      0.95     14293\n\n    accuracy                           0.95     29704\n   macro avg       0.95      0.95      0.95     29704\nweighted avg       0.95      0.95      0.95     29704\n\nConfusion Matrix:\n[[14859   552]\n [  847 13446]]\nROC AUC Score: 0.9897\n\nRandom Forest Evaluation:\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      1.00     15411\n           1       1.00      0.99      1.00     14293\n\n    accuracy                           1.00     29704\n   macro avg       1.00      1.00      1.00     29704\nweighted avg       1.00      1.00      1.00     29704\n\nConfusion Matrix:\n[[15363    48]\n [   87 14206]]\nROC AUC Score: 0.9998\n\nK-Nearest Neighbors Evaluation:\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99     15411\n           1       0.99      0.99      0.99     14293\n\n    accuracy                           0.99     29704\n   macro avg       0.99      0.99      0.99     29704\nweighted avg       0.99      0.99      0.99     29704\n\nConfusion Matrix:\n[[15273   138]\n [  152 14141]]\nROC AUC Score: 0.9974\n\nXGBoost Evaluation:\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      1.00     15411\n           1       1.00      0.99      1.00     14293\n\n    accuracy                           1.00     29704\n   macro avg       1.00      1.00      1.00     29704\nweighted avg       1.00      1.00      1.00     29704\n\nConfusion Matrix:\n[[15370    41]\n [   87 14206]]\nROC AUC Score: 0.9999\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}